{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karenlc4/Spoon-Knife/blob/main/housing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gcsyMJE2BhpF",
        "outputId": "12d041cd-6fab-44a6-8957-528d7a52a668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.5.2\n",
            "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "Successfully installed scikit-learn-1.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn==1.5.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the housing dataset from a URL into a pandas DataFrame\n",
        "housing = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing.csv')\n",
        "\n",
        "# --- Data Exploration ---\n",
        "# Display the first few rows to understand the dataset's structure\n",
        "housing.head()\n",
        "# Show data types and non-null counts for each column\n",
        "housing.info()\n",
        "# Check for missing values in each column\n",
        "housing.isnull().sum()\n",
        "# Generate summary statistics (mean, min, max, etc.) for numerical columns\n",
        "housing.describe()\n",
        "# Visualize the distribution of house prices with a histogram and kernel density estimate\n",
        "sns.histplot(housing['price'], kde=True)\n",
        "\n",
        "# --- Data Cleaning ---\n",
        "# Remove outliers: keep houses with 10 or fewer bedrooms to filter unrealistic data\n",
        "housing = housing[housing['bedrooms'] <= 10]\n",
        "# Remove houses with zero bathrooms, as this is likely invalid data\n",
        "housing = housing[housing['bathrooms'] != 0]\n",
        "\n",
        "# --- Prepare Features and Target ---\n",
        "# Create a copy of the dataset to preserve the original\n",
        "X_full = housing.copy()\n",
        "# Extract the target variable 'price' into y_full\n",
        "y_full = X_full['price']\n",
        "# Drop the 'price' column from X_full to create the feature set\n",
        "X_full.drop(columns='price', inplace=True)\n",
        "\n",
        "# --- Train-Test Split ---\n",
        "# Split data into training (80%) and testing (20%) sets\n",
        "# random_state=42 ensures reproducibility of the split\n",
        "X_train_full, X_test_full, y_train_price, y_test_price = train_test_split(\n",
        "    X_full, y_full, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Feature Engineering Function ---\n",
        "def engineer_features(df, zipcode_price_map=None, fit_scaler=False, scaler=None):\n",
        "    # Create a copy of the input DataFrame to avoid modifying the original\n",
        "    df = df.copy()\n",
        "\n",
        "    # Convert 'date' column to datetime format for easier manipulation\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    # Extract year and month from the sale date as new features\n",
        "    df['year_sold'] = df['date'].dt.year\n",
        "    df['month_sold'] = df['date'].dt.month\n",
        "    # Drop the original 'date' column as it's no longer needed\n",
        "    df.drop(columns='date', inplace=True)\n",
        "\n",
        "    # Create a binary feature indicating if the house was renovated (1 if renovated, 0 if not)\n",
        "    df['was_renovated'] = df['yr_renovated'].apply(lambda x: 1 if x > 0 else 0)\n",
        "    # Drop the original 'yr_renovated' column\n",
        "    df.drop(columns='yr_renovated', inplace=True)\n",
        "\n",
        "    # Add average house price per zipcode as a feature (if zipcode_price_map is provided)\n",
        "    if zipcode_price_map is not None:\n",
        "        df['zipcode_price'] = df['zipcode'].map(zipcode_price_map)\n",
        "    # Drop the 'zipcode' column as it's replaced by zipcode_price\n",
        "    df.drop(columns='zipcode', inplace=True)\n",
        "\n",
        "    # Calculate the age of the house at the time of sale\n",
        "    df['age'] = df['year_sold'] - df['yr_built']\n",
        "    # Drop the 'yr_built' column as it's replaced by 'age'\n",
        "    df.drop(columns='yr_built', inplace=True)\n",
        "\n",
        "    # Drop the 'id' column as it's not useful for prediction\n",
        "    df.drop(columns='id', inplace=True)\n",
        "\n",
        "    # Define columns to scale (numerical features with different ranges)\n",
        "    scale_cols = ['sqft_living', 'sqft_lot', 'sqft_above', 'sqft_basement',\n",
        "                  'sqft_living15', 'sqft_lot15', 'age']\n",
        "\n",
        "    # Scale numerical features to [0,1] range using MinMaxScaler\n",
        "    if fit_scaler:\n",
        "        # For training data: fit the scaler and transform the data\n",
        "        scaler = MinMaxScaler()\n",
        "        df[scale_cols] = scaler.fit_transform(df[scale_cols])\n",
        "    else:\n",
        "        # For test data: apply the trained scaler to transform the data\n",
        "        df[scale_cols] = scaler.transform(df[scale_cols])\n",
        "\n",
        "    # Return the processed DataFrame and the scaler (for use with test data)\n",
        "    return df, scaler\n",
        "\n",
        "# --- Create Zipcode Price Mapping ---\n",
        "# Create a temporary DataFrame with price to calculate average price per zipcode\n",
        "X_train_with_price = X_train_full.copy()\n",
        "X_train_with_price['price'] = y_train_price.values\n",
        "# Compute the mean price for each zipcode in the training data\n",
        "zipcode_price_map = X_train_with_price.groupby('zipcode')['price'].mean()\n",
        "\n",
        "# --- Apply Feature Engineering ---\n",
        "# Process training data and fit the scaler\n",
        "X_train, scaler = engineer_features(X_train_full, zipcode_price_map, fit_scaler=True)\n",
        "# Process test data using the same scaler and zipcode price map\n",
        "X_test, _ = engineer_features(X_test_full, zipcode_price_map, fit_scaler=False, scaler=scaler)\n",
        "\n",
        "# --- Transform Target Variable ---\n",
        "# Apply log transformation to the target variable (price) to reduce skewness\n",
        "y_train = np.log(y_train_price)\n",
        "y_test = np.log(y_test_price)\n",
        "\n",
        "# --- Train XGBoost Model ---\n",
        "# Initialize the XGBoost regressor with specific hyperparameters\n",
        "model = XGBRegressor(\n",
        "    n_estimators=300,       # Number of boosting rounds (trees)\n",
        "    max_depth=5,            # Maximum depth of each tree\n",
        "    learning_rate=0.1,      # Step size for updates during training\n",
        "    subsample=0.8,          # Fraction of samples used per tree\n",
        "    colsample_bytree=0.8,   # Fraction of features used per tree\n",
        "    random_state=42         # Ensure reproducibility\n",
        ")\n",
        "# Train the model on the processed training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# --- Make Predictions and Evaluate ---\n",
        "# Predict on the test set (in log scale)\n",
        "y_pred_log = model.predict(X_test)\n",
        "# Convert predictions back to original scale using exponential\n",
        "y_pred = np.exp(y_pred_log)\n",
        "# Convert true test values back to original scale\n",
        "y_true = np.exp(y_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))  # Root Mean Squared Error\n",
        "mae = mean_absolute_error(y_true, y_pred)           # Mean Absolute Error\n",
        "medae = median_absolute_error(y_true, y_pred)       # Median Absolute Error\n",
        "r2 = r2_score(y_true, y_pred)                       # R-squared score\n",
        "\n",
        "# Print evaluation results\n",
        "print(f\"RMSE: {rmse:.2f}\")          # Measures average prediction error\n",
        "print(f\"MAE: {mae:.2f}\")            # Average absolute prediction error\n",
        "print(f\"Median Abs Error: {medae:.2f}\")  # Median absolute prediction error\n",
        "print(f\"R² Score: {r2:.4f}\")        # Proportion of variance explained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3weXWkLMZHtf",
        "outputId": "fa0ad60f-3710-4dc3-f932-e80e9f4ba616"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 104731.43\n",
            "MAE: 61476.17\n",
            "Median Abs Error: 36314.11\n",
            "R² Score: 0.9137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "holdout = pd.read_csv(\"https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/housing_holdout_test_mini.csv\")\n",
        "\n",
        "# Create a new feature 'zipcode_price' by mapping zipcodes to their average price\n",
        "# Uses the zipcode_price_map created from the training data in the previous code\n",
        "holdout['zipcode_price'] = holdout['zipcode'].map(zipcode_price_map)\n",
        "# Fill missing zipcode prices (for zipcodes not in training data) with the mean of zipcode_price_map\n",
        "# This ensures no missing values in the new feature\n",
        "holdout['zipcode_price'] = holdout['zipcode_price'].fillna(zipcode_price_map.mean())\n",
        "\n",
        "# Apply the same feature engineering as used in training\n",
        "# Uses the engineer_features function from the previous code\n",
        "# fit_scaler=False ensures the scaler (trained on training data) is only applied, not refitted\n",
        "# scaler is the MinMaxScaler object from the training process\n",
        "X_holdout, _ = engineer_features(holdout, zipcode_price_map, fit_scaler=False, scaler=scaler)\n",
        "\n",
        "# Ensure the holdout dataset has the same column order as the training dataset\n",
        "# This is critical because XGBoost expects features in the same order as during training\n",
        "X_holdout = X_holdout[X_train.columns]\n",
        "\n",
        "# Make predictions on the holdout dataset using the trained XGBoost model\n",
        "# Predictions are in log scale (since the model was trained on log-transformed prices)\n",
        "log_predictions = model.predict(X_holdout)\n",
        "# Convert predictions back to the original price scale using exponential\n",
        "predictions = np.exp(log_predictions)\n",
        "\n",
        "# Create a DataFrame with the predictions, with a single column named \"price\"\n",
        "output = pd.DataFrame(predictions, columns=[\"price\"])\n",
        "# Export the predictions to a CSV file named \"team3-module3-predictions.csv\"\n",
        "# index=False prevents writing row indices to the CSV\n",
        "output.to_csv(\"team3-module3-predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "5UgNxk8DU_AZ"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}